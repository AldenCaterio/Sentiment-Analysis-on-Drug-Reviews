{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAlkam/Sentiment-Analysis-on-Drug-Reviews/blob/main/Sentiment_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's load these files and take a look at the data. We'll start with the training set."
      ],
      "metadata": {
        "id": "Of_m8lhW16mN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "fDRRIcdAVSK5",
        "outputId": "3c3c7b55-f84e-472b-d534-1893f19d4304"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1c9b171d-1a38-4abf-95c9-d952f2c2d2bf\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1c9b171d-1a38-4abf-95c9-d952f2c2d2bf\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving drugLibTrain_raw.tsv to drugLibTrain_raw.tsv\n",
            "User uploaded file \"drugLibTrain_raw.tsv\" with length 2282292 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "kBOZlepiW4TR",
        "outputId": "248d3095-047f-4e6e-d65f-1962b6799e70"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-f42aba5d-bcba-4fdb-b681-d640f53df0d9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-f42aba5d-bcba-4fdb-b681-d640f53df0d9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving drugLibTest_raw.tsv to drugLibTest_raw.tsv\n",
            "User uploaded file \"drugLibTest_raw.tsv\" with length 793137 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load data into pandas DataFrames\n",
        "train_df = pd.read_csv('drugLibTrain_raw.tsv', delimiter='\\t')\n",
        "test_df = pd.read_csv('drugLibTest_raw.tsv', delimiter='\\t')"
      ],
      "metadata": {
        "id": "JGoS0tIbV2OJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Let's get an overview of the dataset including the column names, total number of entries, and the data type of each column**"
      ],
      "metadata": {
        "id": "5Jjpu3T53CJF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Information about the training dataset\n",
        "train_df.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vPKjHjZs3Cm2",
        "outputId": "6b13914c-b790-46db-9c21-915fda7f885a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3107 entries, 0 to 3106\n",
            "Data columns (total 9 columns):\n",
            " #   Column             Non-Null Count  Dtype \n",
            "---  ------             --------------  ----- \n",
            " 0   Unnamed: 0         3107 non-null   int64 \n",
            " 1   urlDrugName        3107 non-null   object\n",
            " 2   rating             3107 non-null   int64 \n",
            " 3   effectiveness      3107 non-null   object\n",
            " 4   sideEffects        3107 non-null   object\n",
            " 5   condition          3106 non-null   object\n",
            " 6   benefitsReview     3107 non-null   object\n",
            " 7   sideEffectsReview  3105 non-null   object\n",
            " 8   commentsReview     3099 non-null   object\n",
            "dtypes: int64(2), object(7)\n",
            "memory usage: 218.6+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add sentiment labels**\n",
        "\n",
        "Since the data does not come with predefined sentiment labels, we will need to define our own rules for labeling the sentiments. One approach could be to use the numerical rating feature to infer sentiment:\n",
        "\n",
        "We can define ratings of 7-10 as '**positive**',\n",
        "Ratings of 4-6 as '**neutral**',\n",
        "And ratings of 1-3 as '**negative**'.\n",
        "Alternatively, we could use the effectiveness and sideEffects categorical features to derive sentiment labels. This might be more complex but could potentially provide more nuanced insights. For instance, 'Highly Effective' could be considered 'positive', and 'Severe Side Effects' could be 'negative'.\n"
      ],
      "metadata": {
        "id": "PIZ04vDM2aWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to convert ratings into sentiment labels\n",
        "def rating_to_sentiment(rating):\n",
        "    if rating >= 7:\n",
        "        return 'positive'\n",
        "    elif rating <= 3:\n",
        "        return 'negative'\n",
        "    else:\n",
        "        return 'neutral'\n",
        "\n",
        "# Apply the function to the rating column of both dataframes\n",
        "train_df['sentiment'] = train_df['rating'].apply(rating_to_sentiment)\n",
        "test_df['sentiment'] = test_df['rating'].apply(rating_to_sentiment)"
      ],
      "metadata": {
        "id": "HTMmG39mXBiJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the text**\n",
        "Before running this part, We need to install and import the necessary libraries"
      ],
      "metadata": {
        "id": "xgZPAmpf4aIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OM6gRvcTXOTP",
        "outputId": "68f3b718-f2a0-4d59-a288-ca0c35900f4a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.65.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proceed with the text preprocessing**"
      ],
      "metadata": {
        "id": "mlHdYcXc4xTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a lemmatizer object\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Define the English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Define a function to clean, tokenize, remove stop-words, and lemmatize text\n",
        "def preprocess_text(text):\n",
        "    # Check if the text is not a string (possibly NaN), and if so, replace it with an empty string\n",
        "    if not isinstance(text, str):\n",
        "        text = ''\n",
        "    # Lowercase the text\n",
        "    text = text.lower()\n",
        "    # Remove punctuation, numbers, and special characters\n",
        "    text = re.sub(r'[^a-z ]', '', text)\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords and lemmatize the tokens\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    # Join the tokens back into a single string and return it\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Combine the review columns into a single column and preprocess the text\n",
        "train_df['combined_review'] = (train_df['benefitsReview'].fillna('') + ' ' +\n",
        "                               train_df['sideEffectsReview'].fillna('') + ' ' +\n",
        "                               train_df['commentsReview'].fillna('')).apply(preprocess_text)\n",
        "test_df['combined_review'] = (test_df['benefitsReview'].fillna('') + ' ' +\n",
        "                              test_df['sideEffectsReview'].fillna('') + ' ' +\n",
        "                              test_df['commentsReview'].fillna('')).apply(preprocess_text)"
      ],
      "metadata": {
        "id": "Qre4lNXhXjXL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Exploratory Data Analysis (EDA):**"
      ],
      "metadata": {
        "id": "NhTz4YUaYhP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Distribution of sentiments\n",
        "plt.figure(figsize=(10,5))\n",
        "train_df.sentiment.value_counts().plot(kind='bar')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 492
        },
        "id": "uzMpdHUtYls0",
        "outputId": "e210754a-9e0f-434b-ed92-43d9f32577d9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Vectorization:**"
      ],
      "metadata": {
        "id": "2tK31iRcYql0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# TF-IDF Vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train = vectorizer.fit_transform(train_df['combined_review'])\n",
        "X_test = vectorizer.transform(test_df['combined_review'])\n"
      ],
      "metadata": {
        "id": "3kIFkUEeYxjN"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Model Training and Evaluation: Train a Naive Bayes classifier, predict on the test set, and evaluate the predictions.**"
      ],
      "metadata": {
        "id": "gqltUE1MY2i1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# Label encoding for the target variable\n",
        "y_train = train_df['sentiment'].replace({'positive': 2, 'neutral': 1, 'negative': 0})\n",
        "y_test = test_df['sentiment'].replace({'positive': 2, 'neutral': 1, 'negative': 0})\n",
        "\n",
        "# Naive Bayes Classifier\n",
        "nb_classifier = MultinomialNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = nb_classifier.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rbRWY6iY7eF",
        "outputId": "f397e92c-93ea-44ec-a5d2-0ed3538540b5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.89      0.09      0.16       197\n",
            "           1       0.00      0.00      0.00       169\n",
            "           2       0.66      1.00      0.79       670\n",
            "\n",
            "    accuracy                           0.66      1036\n",
            "   macro avg       0.52      0.36      0.32      1036\n",
            "weighted avg       0.60      0.66      0.54      1036\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The labels 0, 1, and 2 represent 'negative', 'neutral', and 'positive' sentiments, respectively. The columns for precision, recall, and f1-score show the performance of our model for each of these classes.\n",
        "\n",
        "Negative sentiment (0): The model has a high precision (0.89), which means that when it predicts a review has a negative sentiment, it is correct 89% of the time. However, the recall is very low (0.09), which indicates that the model only correctly identified 9% of the actual negative reviews. The f1-score, which balances precision and recall, is low (0.16) due to the low recall. The model seems to struggle with identifying negative reviews.\n",
        "\n",
        "Neutral sentiment (1): The model has a precision, recall, and f1-score of 0. This means that the model did not correctly identify any reviews as neutral. It might be the case that the features distinguishing neutral reviews are not well captured by the model.\n",
        "\n",
        "Positive sentiment (2): The model performs best in identifying positive reviews. It has a precision of 0.66 and a recall of 1.00, indicating that it correctly identified all the positive reviews, but also incorrectly labeled some reviews as positive (hence the precision less than 1). The high f1-score (0.79) reflects this strong performance.\n",
        "\n",
        "The overall accuracy of our model is 0.66, which means it correctly predicts the sentiment 66% of the time.\n",
        "\n",
        "However, given the low performance for negative and neutral classes, we can consider strategies to improve our model's performance. These could include:\n",
        "\n",
        "Feature engineering: we could try to create new features that might help the model distinguish between the classes better. For example, we could use more sophisticated text representation methods like word embeddings (Word2Vec, GloVe), or include metadata features if available (like the length of the review, the drug being reviewed, etc.).\n",
        "\n",
        "Class balancing techniques: If our classes are imbalanced (i.e., there are many more positive reviews than negative or neutral), this could be affecting the model's performance. we could try techniques like oversampling the minority classes or undersampling the majority class.\n",
        "\n",
        "Model tuning: we could try adjusting the parameters of our model to see if we can improve performance. we could also consider trying different models to see if they perform better.\n",
        "\n",
        "Ensemble methods: we could try combining multiple models to make predictions. This often results in better performance than any single model."
      ],
      "metadata": {
        "id": "doYxIUmPlR3o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. The AUC-ROC value**"
      ],
      "metadata": {
        "id": "TWpS42qh9lvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "def multiclass_roc_auc_score(y_test, y_pred, average=\"macro\"):\n",
        "    lb = LabelBinarizer()\n",
        "    lb.fit(y_test)\n",
        "    y_test = lb.transform(y_test)\n",
        "    y_pred = lb.transform(y_pred)\n",
        "    return roc_auc_score(y_test, y_pred, average=average)\n",
        "\n",
        "auc = multiclass_roc_auc_score(y_test, y_pred)\n",
        "print('AUC-ROC:', auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2UwRqnwu9mHv",
        "outputId": "5bd798db-bc95-4028-a39b-e02bb36def56"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC-ROC: 0.521933069428769\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The AUC-ROC value ranges from 0 to 1. An AUC-ROC value of 0.5 indicates that the model has no discrimination capacity to distinguish between positive and negative class. An AUC-ROC value close to 1 signifies that the model has a good measure of separability and is capable of distinguishing between positive and negative classes.\n",
        "\n",
        "The AUC-ROC score of 0.52 suggests that the model's ability to distinguish between the classes is slightly better than random guessing, but it's far from perfect. There's certainly room for improvement."
      ],
      "metadata": {
        "id": "APye1Oj1-BWO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's trained and evaluated a Support Vector Machines (SVM) classifier**"
      ],
      "metadata": {
        "id": "oR0492YZAPIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "# Support Vector Classifier\n",
        "svm_classifier = LinearSVC()\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred_svm = svm_classifier.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(classification_report(y_test, y_pred_svm))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dUbIWSM5lSVK",
        "outputId": "f1f2f0f4-0f28-4553-b0fd-bd89af40cbce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      0.51      0.55       197\n",
            "           1       0.34      0.10      0.16       169\n",
            "           2       0.74      0.90      0.81       670\n",
            "\n",
            "    accuracy                           0.70      1036\n",
            "   macro avg       0.56      0.51      0.51      1036\n",
            "weighted avg       0.65      0.70      0.66      1036\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's interpret the results:\n",
        "\n",
        "Negative sentiment (0): The precision is 0.60, meaning that when the model predicts a review as negative, it is correct 60% of the time. The recall is 0.51, meaning it correctly identified 51% of the actual negative reviews. The F1-score, which balances precision and recall, is 0.55, which is an improvement over the Naive Bayes model.\n",
        "\n",
        "Neutral sentiment (1): The precision is 0.34, indicating that the model correctly identifies a neutral review 34% of the time when it predicts a review to be neutral. The recall is 0.10, meaning the model only correctly identifies 10% of actual neutral reviews. The F1-score is 0.16, which shows that the model still struggles with identifying neutral reviews.\n",
        "\n",
        "Positive sentiment (2): The model performs best in identifying positive reviews, with a precision of 0.74 and a recall of 0.90. The high F1-score of 0.81 reflects this strong performance.\n",
        "\n",
        "The overall accuracy of the model is 0.70, which means it correctly predicts the sentiment 70% of the time. This is an improvement over the Naive Bayes model, which had an accuracy of 0.66.\n",
        "\n",
        "\n",
        "The SVM model seems to be performing better than the Naive Bayes model, especially for negative sentiments. However, both models struggle with the neutral class. This might be because the characteristics of neutral reviews are not well captured by the features, or it could be due to an imbalance in the dataset (if there are fewer neutral reviews).\n",
        "\n",
        "Next steps could include further tuning the SVM model, trying different features, or trying a different type of model. Given that the data contains sequence information (it is text data), a model that can capture this sequential nature, like a Recurrent Neural Network (RNN), might be able to perform better."
      ],
      "metadata": {
        "id": "SMui_vQmmioh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've trained and evaluated a SVM model, the next step is to experiment with a deep learning model - Recurrent Neural Network (RNN) model for our sentiment analysis task.\n",
        "\n",
        "RNNs are a type of neural network that are great for sequential data, like text, because they have \"memory\" - they take as their input not just the current input, but also what they have perceived previously in time.\n",
        "\n",
        "However, setting up and training an RNN (especially on text data) can be quite complex and computationally expensive. It involves several additional steps such as:\n",
        "\n",
        "**Tokenization:** Converting the text into a sequence of integers (tokens).\n",
        "**Padding:** Making sure all the sequences are the same length by padding shorter ones with zeros.\n",
        "**Creating an embedding layer:** This is a layer in the neural network that converts the tokens into dense vectors of fixed size that the network can learn from. It's a way to reduce the dimensionality of the input data. **Building the RNN model:** This involves defining the architecture of the model. For example, we use an LSTM (Long Short-Term Memory) or GRU (Gated Recurrent Unit) layer, which are special types of RNN layers that can better capture long-term dependencies in the data.\n",
        "**Training the model:** This is the process of feeding our data through the network and updating the weights of the network to minimize the prediction error.\n",
        "**Evaluating the model:** This is similar to what we did with the SVM and Naive Bayes models - comparing the model's predictions on a test set to the actual labels to see how well the model is performing.\n",
        "\n",
        "All these steps require specific libraries and functions. In Python, the Keras library (which is part of TensorFlow) is commonly used for building and training RNNs."
      ],
      "metadata": {
        "id": "6Jo_YU2vBKMK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Tensoflow**"
      ],
      "metadata": {
        "id": "E_TTnqueCg5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VH9XxuC1mjEd",
        "outputId": "b1824820-5835-46e4-acaf-b4e3b6fe22f5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.13)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Keras**"
      ],
      "metadata": {
        "id": "a_V5bSq7CocS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras tensorflow\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYSL9ChInz2w",
        "outputId": "dfff3231-3d9e-4017-a885-e9ef7550ec93"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.56.2)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.13)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.3)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.7.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Let's train an RNN model using Keras**\n",
        "\n",
        "Let's train the Long Short-Term Memory (LSTM) model which is a type of Recurrent Neural Network (RNN)."
      ],
      "metadata": {
        "id": "V9iKMuKHrdwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Maximum number of words to keep based on word frequency\n",
        "max_words = 5000\n",
        "\n",
        "# Maximum length of the sequences\n",
        "max_len = 100\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(train_df['combined_review'])\n",
        "sequences_train = tokenizer.texts_to_sequences(train_df['combined_review'])\n",
        "sequences_test = tokenizer.texts_to_sequences(test_df['combined_review'])\n",
        "\n",
        "# Padding\n",
        "X_train = pad_sequences(sequences_train, maxlen=max_len)\n",
        "X_test = pad_sequences(sequences_test, maxlen=max_len)\n",
        "\n",
        "# Labels\n",
        "y_train = train_df['sentiment'].replace({'positive': 2, 'neutral': 1, 'negative': 0}).values\n",
        "y_test = test_df['sentiment'].replace({'positive': 2, 'neutral': 1, 'negative': 0}).values\n",
        "\n",
        "# Building the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))  # 3 classes: positive, neutral, negative\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Training the model\n",
        "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluating the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xdv0oKTRrcFE",
        "outputId": "9fc46d7b-e77d-4485-98f2-890ece17d3fb"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "98/98 [==============================] - 20s 158ms/step - loss: 0.8753 - accuracy: 0.6814 - val_loss: 0.8429 - val_accuracy: 0.6467\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 8s 78ms/step - loss: 0.6979 - accuracy: 0.7296 - val_loss: 0.8105 - val_accuracy: 0.6940\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 7s 67ms/step - loss: 0.5295 - accuracy: 0.7924 - val_loss: 0.9072 - val_accuracy: 0.6882\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 8s 79ms/step - loss: 0.3867 - accuracy: 0.8384 - val_loss: 1.0087 - val_accuracy: 0.6554\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 7s 67ms/step - loss: 0.2820 - accuracy: 0.8912 - val_loss: 1.5724 - val_accuracy: 0.6651\n",
            "33/33 [==============================] - 1s 15ms/step - loss: 1.5724 - accuracy: 0.6651\n",
            "Test Loss: 1.5724167823791504\n",
            "Test Accuracy: 0.665057897567749\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Here's a breakdown of the output:**\n",
        "\n",
        "**Epochs:** we trained the model for 5 epochs. In each epoch, the model went through the entire training dataset once.\n",
        "\n",
        "**Training Loss and Accuracy:** These values represent how well our model is doing on the training data. The training loss decreased with each epoch, which is a good sign. It means the model's predictions are getting closer to the actual values. The accuracy increased with each epoch, which indicates the model is correctly classifying a higher percentage of reviews.\n",
        "\n",
        "**Validation Loss and Accuracy:** These values represent how well our model is expected to perform on unseen data, based on its performance on the validation set. Ideally, we want the validation loss to decrease and the validation accuracy to increase, just like the training metrics. However, in this case, the validation loss increased and the validation accuracy didn't improve significantly. This may indicate overfitting, meaning the model is fitting the training data too closely and not generalizing well to new data.\n",
        "\n",
        "**Test Loss and Accuracy:** After training, we evaluated the model on the test set, which it hadn't seen before. The test loss and accuracy give us an unbiased estimate of how our model will perform on new data. In this case, the test accuracy is approximately 67.7%, meaning the model correctly classified about 67.7% of the reviews in the test set.\n",
        "\n",
        "Overall, our LSTM model has learned to classify the sentiment of drug reviews with a reasonable degree of accuracy. However, there's likely room for improvement. At this time, we want to experiment with different model architectures, add regularization techniques (like dropout), or try different hyperparameters to improve the model's performance."
      ],
      "metadata": {
        "id": "DEIQE9ObtQIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add More LSTM Layers:** Stacking LSTM layers can often lead to better performance. We can add another LSTM layer."
      ],
      "metadata": {
        "id": "By5nFJuzHZMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "model.add(LSTM(64, return_sequences=True))  # return_sequences must be True for stacking\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "r6TJwzK2tQrD"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Change the LSTM to a Bidirectional LSTM:** Bidirectional LSTMs can capture patterns from both the beginning and end of a sequence, which can be helpful in understanding the context."
      ],
      "metadata": {
        "id": "Gl9yq1SdtShI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "model.add(Bidirectional(LSTM(64)))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "_KSj7ZoOtS4F"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use a Different Type of RNN:** We try using a Gated Recurrent Unit (GRU) instead of an LSTM. GRUs are a variation of LSTMs that are a bit simpler and can sometimes perform just as well."
      ],
      "metadata": {
        "id": "hA_TsiSwti2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "model.add(GRU(64))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "_P75JqUrtjMv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Add More Dense Layers:** We also add more Dense layers to our model. This can sometimes improve performance, but it may also increase the risk of overfitting."
      ],
      "metadata": {
        "id": "NKH5UGntts4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(64, activation='relu'))  # additional Dense layer\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "esdEbdTottTs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Adding an extra LSTM layer: Update Model Architecture:**"
      ],
      "metadata": {
        "id": "jLEh3foguTBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Build the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=max_words, output_dim=64, input_length=max_len))\n",
        "model.add(LSTM(64, return_sequences=True))  # First LSTM layer with return_sequences=True\n",
        "model.add(LSTM(64))  # Second LSTM layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))  # 3 classes: positive, neutral, negative\n"
      ],
      "metadata": {
        "id": "3bcgqf73uUig"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the Model:**"
      ],
      "metadata": {
        "id": "8SOQ-YTcuZIH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTU0vGpQufht",
        "outputId": "ce24da4a-786d-4df0-fe56-5ef8b3dee84a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "98/98 [==============================] - 19s 146ms/step - loss: 0.8641 - accuracy: 0.6810 - val_loss: 0.8093 - val_accuracy: 0.6680\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 13s 137ms/step - loss: 0.6581 - accuracy: 0.7467 - val_loss: 0.7952 - val_accuracy: 0.6979\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 14s 141ms/step - loss: 0.4860 - accuracy: 0.8156 - val_loss: 0.9160 - val_accuracy: 0.6786\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 14s 141ms/step - loss: 0.3777 - accuracy: 0.8462 - val_loss: 0.9776 - val_accuracy: 0.6458\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 14s 143ms/step - loss: 0.2805 - accuracy: 0.8886 - val_loss: 1.2629 - val_accuracy: 0.6149\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7df6fca2dea0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the Model:**"
      ],
      "metadata": {
        "id": "eRr3PnwfvGI0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print('Test Loss:', loss)\n",
        "print('Test Accuracy:', accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCB6SiwTvHt4",
        "outputId": "94416fd4-c4ba-43b0-ee94-a630f777f98c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 1s 34ms/step - loss: 1.2629 - accuracy: 0.6149\n",
            "Test Loss: 1.2629008293151855\n",
            "Test Accuracy: 0.6148648858070374\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test accuracy is approximately 63.51%, meaning the model correctly classified about 63.51% of the reviews in the test set. This is a slight worsening from our previous LSTM model, which had an accuracy of approximately 67.67%.  we can do multiple iterations of model training, tuning, and evaluation, but we do not do it now."
      ],
      "metadata": {
        "id": "PlbI1xk41bRZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Feature Engineering:**\n",
        "\n",
        "For using word embeddings (like Word2Vec or GloVe), we can use the gensim library. The embeddings can be used to replace the Embedding layer in our model. Here's a simple example with Word2Vec:"
      ],
      "metadata": {
        "id": "J-Htq7ldKSDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aoAx33d-FbfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4FUZ9VeA9pD0",
        "outputId": "59b35dfa-2e5f-49e2-f2f0-0ad1ca2aa14e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "RS82oCKDFeks"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pg8CXF5CF7Tb",
        "outputId": "d540e6e7-03c5-411c-efec-c5ccad57f8bd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Unnamed: 0', 'urlDrugName', 'rating', 'effectiveness', 'sideEffects',\n",
            "       'condition', 'benefitsReview', 'sideEffectsReview', 'commentsReview',\n",
            "       'sentiment', 'combined_review'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = train_df['combined_review'].values.tolist()\n"
      ],
      "metadata": {
        "id": "1QETh3gMFs0F"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_reviews = [review.lower().split() for review in reviews]\n"
      ],
      "metadata": {
        "id": "7ePKd5wrGi0C"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = train_df['combined_review'].values.tolist()\n",
        "decoded_reviews = [review.lower().split() for review in reviews]\n",
        "w2v_model = Word2Vec(decoded_reviews, vector_size=64, window=5, min_count=1, workers=4)\n",
        "w2v_model.train(decoded_reviews, total_examples=w2v_model.corpus_count, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTOsMaEgGmEh",
        "outputId": "331ae2bf-073f-44cb-8f9f-6ebb17ec57d7"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1721681, 1897610)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_len))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Train a Word2Vec model on our corpus\n",
        "w2v_model = Word2Vec(decoded_reviews, vector_size=64, window=5, min_count=1, workers=4)\n",
        "w2v_model.train(decoded_reviews, total_examples=w2v_model.corpus_count, epochs=10)\n",
        "\n",
        "# Get the word vectors from the trained Word2Vec model\n",
        "embedding_matrix = np.zeros((5000, 64))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < 5000:\n",
        "        if word in w2v_model.wv:\n",
        "            embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Use the word vectors as weights in our Embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlICXMKvMEdA",
        "outputId": "b82be405-a412-4cbc-e69f-b9c8b92a9651"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_len))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "# Train a Word2Vec model on our corpus\n",
        "w2v_model = Word2Vec(decoded_reviews, vector_size=64, window=5, min_count=1, workers=4)\n",
        "w2v_model.train(decoded_reviews, total_examples=w2v_model.corpus_count, epochs=10)\n",
        "\n",
        "# Get the word vectors from the trained Word2Vec model\n",
        "embedding_matrix = np.zeros((5000, 64))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < 5000:\n",
        "        if word in w2v_model.wv:\n",
        "            embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Use the word vectors as weights in our Embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wlshA6yRLvE7",
        "outputId": "7e6414a2-03eb-407e-aec7-7bab62b46e10"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Convert our labels to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=5, validation_data=(X_test, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLW-WwwGMlnl",
        "outputId": "c3bc2129-0b00-4879-8992-a58d3bef0ddf"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "98/98 [==============================] - 9s 68ms/step - loss: 0.8616 - accuracy: 0.6801 - val_loss: 0.8529 - val_accuracy: 0.6467\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 5s 50ms/step - loss: 0.8168 - accuracy: 0.6884 - val_loss: 0.8333 - val_accuracy: 0.6477\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 7s 75ms/step - loss: 0.7948 - accuracy: 0.6907 - val_loss: 0.8211 - val_accuracy: 0.6515\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 5s 52ms/step - loss: 0.7729 - accuracy: 0.6968 - val_loss: 0.8147 - val_accuracy: 0.6622\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 6s 59ms/step - loss: 0.7533 - accuracy: 0.7081 - val_loss: 0.7945 - val_accuracy: 0.6776\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7df6f1a1e110>"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assume `new_review` is our new text data\n",
        "new_review = [\"This is a review of the drug\"]\n",
        "\n",
        "# Tokenize the new review\n",
        "sequences_new = tokenizer.texts_to_sequences(new_review)\n",
        "\n",
        "# Pad the new review\n",
        "padded_new = pad_sequences(sequences_new, maxlen=max_len, padding='post', truncating='post')\n",
        "\n",
        "# Make a prediction\n",
        "pred = model.predict(padded_new)\n",
        "\n",
        "# The prediction will be a 3-element vector representing the probabilities of the review being 'negative', 'neutral', and 'positive'.\n",
        "# To get the class with the highest probability, we can use the `np.argmax()` function.\n",
        "sentiment = np.argmax(pred)\n",
        "\n",
        "print(sentiment)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uo8YUO7mNJia",
        "outputId": "c06e6cd5-a570-4e81-e98d-cb42b665c3c9"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 453ms/step\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_df['combined_review'] = train_df['benefitsReview'] + \" \" + train_df['sideEffectsReview'] + \" \" + train_df['commentsReview']\n",
        "test_df['combined_review'] = test_df['benefitsReview'] + \" \" + test_df['sideEffectsReview'] + \" \" + test_df['commentsReview']\n"
      ],
      "metadata": {
        "id": "KxeSs8McPFmw"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['combined_review'].isna().sum())\n",
        "print(test_df['combined_review'].isna().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ch_6a-x7QK2_",
        "outputId": "02ac8f09-2466-4730-9f18-dd871313e549"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the length of reviews\n",
        "review_length = [len(review) for review in sequences_train]\n",
        "\n",
        "# Find the maximum length\n",
        "max_len = max(review_length)\n",
        "\n",
        "# Now pad the sequences\n",
        "X_train_pad = pad_sequences(sequences_train, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(sequences_test, maxlen=max_len, padding='post')\n"
      ],
      "metadata": {
        "id": "eAuoj3PjS-Cv"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'combined_review' column to string\n",
        "train_df['combined_review'] = train_df['combined_review'].astype(str)\n",
        "test_df['combined_review'] = test_df['combined_review'].astype(str)\n",
        "\n",
        "# Now, tokenize and pad the training data\n",
        "sequences_train = tokenizer.texts_to_sequences(train_df['combined_review'])\n",
        "X_train_pad = pad_sequences(sequences_train, maxlen=max_len, padding='post')\n",
        "\n",
        "# Tokenize and pad the test data\n",
        "sequences_test = tokenizer.texts_to_sequences(test_df['combined_review'])\n",
        "X_test_pad = pad_sequences(sequences_test, maxlen=max_len, padding='post')\n",
        "\n",
        "# Replace 'y_train' and 'y_test' with our labels\n",
        "y_train = train_df['sentiment'].replace({'positive': 2, 'neutral': 1, 'negative': 0})\n",
        "y_test = test_df['sentiment'].replace({'positive': 2, 'neutral': 1, 'negative': 0})\n"
      ],
      "metadata": {
        "id": "mcSWIaJ1SvpI"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required library\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Convert the sequences back to text\n",
        "word_index = tokenizer.word_index\n",
        "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "decoded_reviews = [[reverse_word_index.get(i, '?') for i in sequence] for sequence in sequences_train]\n",
        "\n",
        "# Train the Word2Vec model\n",
        "w2v_model = Word2Vec(decoded_reviews, vector_size=64, window=5, min_count=1, workers=4)\n",
        "w2v_model.train(decoded_reviews, total_examples=w2v_model.corpus_count, epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbvKVMR8QNP-",
        "outputId": "91418172-6740-40d0-eba2-65bcb953dd10"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Effective 'alpha' higher than previous training cycles\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1664486, 3695940)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the LSTM model:**\n",
        "Now, we will build the LSTM model that will use the word embeddings from the trained Word2Vec model."
      ],
      "metadata": {
        "id": "hQ2pEnpPRCxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_len))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "rgJN5bFLQ6xt"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the weights in the Embedding layer:**\n",
        "We now set the weights in the Embedding layer of the LSTM model to the word vectors obtained from the Word2Vec model."
      ],
      "metadata": {
        "id": "vvvcfLxNROfG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the word vectors from the trained Word2Vec model\n",
        "embedding_matrix = np.zeros((5000, 64))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if i < 5000:\n",
        "        if word in w2v_model.wv:\n",
        "            embedding_matrix[i] = w2v_model.wv[word]\n",
        "\n",
        "# Use the word vectors as weights in the Embedding layer\n",
        "model.layers[0].set_weights([embedding_matrix])\n",
        "model.layers[0].trainable = False\n"
      ],
      "metadata": {
        "id": "hMZuVqp9RgCZ"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile and train the LSTM model:**\n",
        "Finally, We compile the LSTM model and train it using the tokenized and padded training data."
      ],
      "metadata": {
        "id": "UgAS640xRl1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary library\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Define the maximum length of a sequence\n",
        "max_len = 100\n",
        "\n",
        "# Tokenize and pad the training data\n",
        "sequences_train = tokenizer.texts_to_sequences(train_df['combined_review'])\n",
        "X_train_pad = pad_sequences(sequences_train, maxlen=max_len, padding='post')\n",
        "\n",
        "# Tokenize and pad the test data\n",
        "sequences_test = tokenizer.texts_to_sequences(test_df['combined_review'])\n",
        "X_test_pad = pad_sequences(sequences_test, maxlen=max_len, padding='post')\n",
        "\n",
        "# Replace 'y_train' and 'y_test' with our labels\n",
        "y_train = train_df['sentiment'].replace({'positive': 2, 'neutral': 1, 'negative': 0})\n",
        "y_test = test_df['sentiment'].replace({'positive': 2, 'neutral': 1, 'negative': 0})\n"
      ],
      "metadata": {
        "id": "TQSlJgvYSNVI"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Find the length of reviews\n",
        "review_length = [len(review) for review in sequences_train]\n",
        "\n",
        "# Find the maximum length\n",
        "max_len = max(review_length)\n",
        "\n",
        "# Now pad the sequences\n",
        "X_train_pad = pad_sequences(sequences_train, maxlen=max_len, padding='post')\n",
        "X_test_pad = pad_sequences(sequences_test, maxlen=max_len, padding='post')\n",
        "\n",
        "# Define the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=5000, output_dim=64, input_length=max_len))  # Ensure `input_length=max_len`\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(3, activation='softmax'))\n"
      ],
      "metadata": {
        "id": "Or1kciFFWa6l"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train_pad, y_train, epochs=5, validation_data=(X_test_pad, y_test))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6zVJt2ORvtn",
        "outputId": "7d13651e-9c4c-4727-e94d-c6e3a54c6f9e"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "98/98 [==============================] - 67s 656ms/step - loss: 0.8719 - accuracy: 0.6823 - val_loss: 0.8938 - val_accuracy: 0.6467\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 63s 646ms/step - loss: 0.8546 - accuracy: 0.6855 - val_loss: 0.8939 - val_accuracy: 0.6467\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 62s 631ms/step - loss: 0.8572 - accuracy: 0.6855 - val_loss: 0.8998 - val_accuracy: 0.6467\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 63s 639ms/step - loss: 0.8488 - accuracy: 0.6855 - val_loss: 0.8937 - val_accuracy: 0.6467\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 64s 654ms/step - loss: 0.8518 - accuracy: 0.6855 - val_loss: 0.8935 - val_accuracy: 0.6467\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7df6eb52e620>"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model:**\n",
        "After training the model, we can evaluate its performance on the test data."
      ],
      "metadata": {
        "id": "oj97QKzkYvmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss, test_accuracy = model.evaluate(X_test_pad, y_test)\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_accuracy))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yxVXUTcpYwBD",
        "outputId": "2f23d63a-86a0-4698-bce8-029148906d7e"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 4s 125ms/step - loss: 0.8935 - accuracy: 0.6467\n",
            "Test Loss: 0.8934745192527771\n",
            "Test Accuracy: 0.6467181444168091\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It appears that our model has achieved an accuracy of approximately 64.67% on the test set, which is a reasonable starting point.\n",
        "\n",
        "However, the loss is still relatively high, which might suggest that the model could be further improved."
      ],
      "metadata": {
        "id": "iZvNMuUfZUK_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lsWpAP9WJPYf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dkhmCgApJPwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Assuming y_pred_probs is our predicted probabilities for the test set\n",
        "y_pred_probs = model.predict(X_test_pad)\n",
        "\n",
        "# Use np.argmax to get class labels\n",
        "y_pred = np.argmax(y_pred_probs, axis=-1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ocHwitfJKY5-",
        "outputId": "5824109c-25c2-49c0-8e4d-1de8d00d8b17"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "33/33 [==============================] - 4s 119ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Compute precision, recall, and F1-score\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(report)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1mFqZ9qKgHL",
        "outputId": "e9e553d9-e9bf-455d-b9f0-18e66d9f8421"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       197\n",
            "           1       0.00      0.00      0.00       169\n",
            "           2       0.65      1.00      0.79       670\n",
            "\n",
            "    accuracy                           0.65      1036\n",
            "   macro avg       0.22      0.33      0.26      1036\n",
            "weighted avg       0.42      0.65      0.51      1036\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The results show that our model is classifying almost everything into the third class (label 2). This is evident from the recall of 1.00 for class 2, and 0.00 for the other two classes. It's likely that our model is not learning meaningful features to distinguish between the classes, and is instead defaulting to the most common class."
      ],
      "metadata": {
        "id": "OlM2t_IfYTkq"
      }
    }
  ]
}